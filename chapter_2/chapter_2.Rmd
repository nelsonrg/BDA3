---
title: "Chapter 2"
author: "Bobby Nelson"
date: "2024-08-16"
output:
  html_document:
    theme: default
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
header-includes:
  - \newcommand{\E}{\operatorname{E}}
  - \newcommand{\Var}{\operatorname{Var}}
  - \newcommand{\Cov}{\operatorname{Cov}}
  - \newcommand{\Pr}{\operatorname{Pr}}
  - \usepackage{amsmath}
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(latex2exp)
```

# Chapter Notes

## Derive Equation 2.6

---

The authors leave the following integration problem as an exercise to the reader.

**For posterior prediction from this model, we might be more interested in the outcome of one new trial, rather than another set of $n$ new trials. Letting $\tilde{y}$ denote the result of a new trial, exchangeable with the first $n$,**

$$
\begin{align*}
  \Pr(\tilde{y}=1 \mid y)
    &= \int_0^1 \Pr(\tilde{y}=1 \mid \theta, y) p(\theta \mid y) d \theta \\
    &= \int_0^1 \theta p(\theta \mid y) d \theta \\
    &= E(\theta \mid y) \\
    &= \frac{y + 1}{n + 2}
\end{align*}
$$

**from the properties of the beta distribution.**

We start by specifying that $p(\theta \mid y) \sim {n \choose y} \theta^y (1 - \theta)^{n-y}$. This is equivalent to $\text{Beta}(y + 1, n + 1 - y)$.

Next we substitute the gamma notation for the binomial coefficient ${n \choose y} = \frac{\Gamma{(n+2)}}{\Gamma{(y+1)} \ \Gamma{(n+1-y)}}$.

So we have

$$
\begin{align*}
  E(\theta \mid y)
    &= \int_0^1 \theta \frac{\Gamma{(n+2)}}{\Gamma{(y+1)} \ \Gamma{(n+1-y)}} \theta^y (1 - \theta)^{n-y} d \theta \\
    &= \int_0^1 \frac{\Gamma{(n+2)}}{\Gamma{(y+1)} \ \Gamma{(n+1-y)}} \theta^{y+1} (1 - \theta)^{n-y} d \theta 
\end{align*}
$$

We now want to manipulate the expression so that the terms inside the integral are a Beta distribution with $\alpha = y + 2$ and $\beta = n + 1 - y$. The Beta coefficient for these terms will be $B(y + 2, n + 1 - y) = \frac{\Gamma{(y+2)} \ \Gamma{(n+1-y)}}{\Gamma{(n+3)}}$. The integral of any Beta distribution from 0 to 1 is 1.

$$
\begin{align*}
  E(\theta \mid y)
    &= \int_0^1 \frac{\Gamma{(n+2)}}{\Gamma{(y+1)} \ \Gamma{(n+1-y)}} \theta^{y+1} (1 - \theta)^{n-y} d \theta \\
    &= \frac{\Gamma{(y+2)}}{\Gamma{(y+1)}} \frac{\Gamma{(n+2)}}{\Gamma{(n+3)}}
        \int_0^1 \frac{\Gamma{(n+3)}}{\Gamma{(y+2)} \ \Gamma{(n+1-y)}} \theta^{y+1} (1 - \theta)^{n-y} d \theta \\
    &= \frac{\Gamma{(y+2)}}{\Gamma{(y+1)}} \frac{\Gamma{(n+2)}}{\Gamma{(n+3)}} \times 1
\end{align*}
$$

Recall that $\Gamma{(x+1)} = x \Gamma{(x)}$ to simplify the expression.

$$
\begin{align*}
  E(\theta \mid y)
    &= \frac{\Gamma{(y+2)}}{\Gamma{(y+1)}} \frac{\Gamma{(n+2)}}{\Gamma{(n+3)}} \\
    &= \frac{(y+1) \Gamma{(y+1)}}{\Gamma{(y+1)}} \frac{\Gamma{(n+2)}}{(n+2)\Gamma{(n+2)}} \\
    &= \frac{y+1}{n+2}
\end{align*}
$$

## Show that the binomial is in the exponential family

---

From the chapter, exponential families are of the form

$$
\begin{equation}
  \label{eq:expfamily}
  p(y \mid \theta) = f(y) g(\theta)^n e^{\phi(\theta)^T t(y)}
\end{equation}
$$

Start with the binomial distribution and collect the terms by their exponent.

$$
\begin{align*}
  p(y \mid \theta, n)
    &= {n \choose y} \theta^y (1-\theta)^{n-y} \\
    &= {n \choose y} \theta^y (1-\theta)^{-y} (1-\theta)^n \\
    &= {n \choose y} (1-\theta)^n \left( \frac{\theta}{1-\theta} \right)^y
\end{align*}
$$

Now put the last term in an exponent.

$$
\begin{align*}
  p(y \mid \theta, n)
    &= {n \choose y} (1-\theta)^n \left( \frac{\theta}{1-\theta} \right)^y \\
    &= {n \choose y} (1-\theta)^n e^{y \log{\frac{\theta}{1-\theta}}}
\end{align*}
$$

This is in the form of the exponential family as given in Equation $\eqref{eq:expfamily}$.

$$
\begin{align*}
  f(y) &= {n \choose y} \\
  g(\theta) &= (1 - \theta) \\
  \phi(\theta) &= \log{\frac{\theta}{1 - \theta}} \\
  t(y) &= y
\end{align*}
$$

# Exercises

## 1

---

**Posterior inference: suppose you have a Beta(4, 4) prior distribution on the probability θ that a coin will yield a ‘head’ when spun in a specified manner. The coin is independently spun ten times, and ‘heads’ appear fewer than 3 times. You are not told how many heads were seen, only that the number is less than 3. Calculate your exact posterior density (up to a proportionality constant) for θ and sketch it.**

The prior density is

$$
p(\theta) = \frac{1}{B(4, 4)} \theta^3(1-\theta)^3
$$

The likelihood of fewer than 3 heads is the cumulative mass function from $y = 0$ to $y = 2$.

$$
\begin{align*}
  p(y \mid \theta)
    &= {10 \choose 0} \theta^0(1-\theta)^{10} + 
       {10 \choose 1} \theta^1(1-\theta)^{9} +
       {10 \choose 2} \theta^2(1-\theta)^{8} \\
    &= (1-\theta)^{10} +
       10 \theta (1-\theta)^9 +
       45 \theta^2 (1-\theta)^8
\end{align*}
$$

So the posterior density is

$$
\begin{align*}
  p(\theta \mid y) &= p(\theta) p(y \mid \theta) \\
    &\propto \theta^3(1-\theta)^{13} +
             10 \theta^4 (1-\theta)^{12} +
             45 \theta^5 (1-\theta)^{11}
\end{align*}
$$

```{r}
theta <- seq(0, 1, length.out = 100)
prior <- dbeta(theta, 4, 4)
posterior <- (dbinom(0, 10, theta) +
              dbinom(1, 10, theta) +
              dbinom(2, 10, theta)) * prior
posterior <- posterior * sum(prior) / sum(posterior)  # approximate scaling
plot(
  x = theta,
  y = prior,
  type = "l",
  lty = "dashed",
  xlab = TeX(r"($\theta$)"),
  ylab = TeX(r"($p(\theta)$)"),
  ylim = c(0, ceiling(max(prior, posterior)))
)
lines(
  x = theta,
  y = posterior,
  lty = "solid"
)
legend(
  x = "topright",
  lty = c("dashed", "solid"),
  legend = c("Prior", "Posterior")
)
```



