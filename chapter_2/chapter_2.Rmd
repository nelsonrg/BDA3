---
title: "Chapter 2"
author: "Bobby Nelson"
date: "2024-08-16"
output:
  html_document:
    theme: default
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
header-includes:
  - \newcommand{\E}{\operatorname{E}}
  - \newcommand{\Var}{\operatorname{Var}}
  - \newcommand{\Cov}{\operatorname{Cov}}
  - \newcommand{\Pr}{\operatorname{Pr}}
  - \usepackage{amsmath}
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(purrr)
library(dplyr)
library(ggplot2)
library(latex2exp)
library(kableExtra)
```

# Chapter Notes

## Derive Equation 2.6

---

The authors leave the following integration problem as an exercise to the reader.

**For posterior prediction from this model, we might be more interested in the outcome of one new trial, rather than another set of $n$ new trials. Letting $\tilde{y}$ denote the result of a new trial, exchangeable with the first $n$,**

$$
\begin{align*}
  \Pr(\tilde{y}=1 \mid y)
    &= \int_0^1 \Pr(\tilde{y}=1 \mid \theta, y) p(\theta \mid y) d \theta \\
    &= \int_0^1 \theta p(\theta \mid y) d \theta \\
    &= E(\theta \mid y) \\
    &= \frac{y + 1}{n + 2}
\end{align*}
$$

**from the properties of the beta distribution.**

We start by specifying that $p(\theta \mid y) \sim {n \choose y} \theta^y (1 - \theta)^{n-y}$. This is equivalent to $\text{Beta}(y + 1, n + 1 - y)$.

Next we substitute the gamma notation for the binomial coefficient ${n \choose y} = \frac{\Gamma{(n+2)}}{\Gamma{(y+1)} \ \Gamma{(n+1-y)}}$.

So we have

$$
\begin{align*}
  E(\theta \mid y)
    &= \int_0^1 \theta \frac{\Gamma{(n+2)}}{\Gamma{(y+1)} \ \Gamma{(n+1-y)}} \theta^y (1 - \theta)^{n-y} d \theta \\
    &= \int_0^1 \frac{\Gamma{(n+2)}}{\Gamma{(y+1)} \ \Gamma{(n+1-y)}} \theta^{y+1} (1 - \theta)^{n-y} d \theta 
\end{align*}
$$

We now want to manipulate the expression so that the terms inside the integral are a Beta distribution with $\alpha = y + 2$ and $\beta = n + 1 - y$. The Beta coefficient for these terms will be $B(y + 2, n + 1 - y) = \frac{\Gamma{(y+2)} \ \Gamma{(n+1-y)}}{\Gamma{(n+3)}}$. The integral of any Beta distribution from 0 to 1 is 1.

$$
\begin{align*}
  E(\theta \mid y)
    &= \int_0^1 \frac{\Gamma{(n+2)}}{\Gamma{(y+1)} \ \Gamma{(n+1-y)}} \theta^{y+1} (1 - \theta)^{n-y} d \theta \\
    &= \frac{\Gamma{(y+2)}}{\Gamma{(y+1)}} \frac{\Gamma{(n+2)}}{\Gamma{(n+3)}}
        \int_0^1 \frac{\Gamma{(n+3)}}{\Gamma{(y+2)} \ \Gamma{(n+1-y)}} \theta^{y+1} (1 - \theta)^{n-y} d \theta \\
    &= \frac{\Gamma{(y+2)}}{\Gamma{(y+1)}} \frac{\Gamma{(n+2)}}{\Gamma{(n+3)}} \times 1
\end{align*}
$$

Recall that $\Gamma{(x+1)} = x \Gamma{(x)}$ to simplify the expression.

$$
\begin{align*}
  E(\theta \mid y)
    &= \frac{\Gamma{(y+2)}}{\Gamma{(y+1)}} \frac{\Gamma{(n+2)}}{\Gamma{(n+3)}} \\
    &= \frac{(y+1) \Gamma{(y+1)}}{\Gamma{(y+1)}} \frac{\Gamma{(n+2)}}{(n+2)\Gamma{(n+2)}} \\
    &= \frac{y+1}{n+2}
\end{align*}
$$

## Show that the binomial is in the exponential family

---

From the chapter, exponential families are of the form

$$
\begin{equation}
  \label{eq:expfamily}
  p(y \mid \theta) = f(y) g(\theta)^n e^{\phi(\theta)^T t(y)}
\end{equation}
$$

Start with the binomial distribution and collect the terms by their exponent.

$$
\begin{align*}
  p(y \mid \theta, n)
    &= {n \choose y} \theta^y (1-\theta)^{n-y} \\
    &= {n \choose y} \theta^y (1-\theta)^{-y} (1-\theta)^n \\
    &= {n \choose y} (1-\theta)^n \left( \frac{\theta}{1-\theta} \right)^y
\end{align*}
$$

Now put the last term in an exponent.

$$
\begin{align*}
  p(y \mid \theta, n)
    &= {n \choose y} (1-\theta)^n \left( \frac{\theta}{1-\theta} \right)^y \\
    &= {n \choose y} (1-\theta)^n e^{y \log{\frac{\theta}{1-\theta}}}
\end{align*}
$$

This is in the form of the exponential family as given in Equation $\eqref{eq:expfamily}$.

$$
\begin{align*}
  f(y) &= {n \choose y} \\
  g(\theta) &= (1 - \theta) \\
  \phi(\theta) &= \log{\frac{\theta}{1 - \theta}} \\
  t(y) &= y
\end{align*}
$$

# Exercises

## 1

---

**Posterior inference: suppose you have a Beta(4, 4) prior distribution on the probability θ that a coin will yield a ‘head’ when spun in a specified manner. The coin is independently spun ten times, and ‘heads’ appear fewer than 3 times. You are not told how many heads were seen, only that the number is less than 3. Calculate your exact posterior density (up to a proportionality constant) for θ and sketch it.**

The prior density is

$$
p(\theta) = \frac{1}{B(4, 4)} \theta^3(1-\theta)^3
$$

The likelihood of fewer than 3 heads is the cumulative mass function from $y = 0$ to $y = 2$.

$$
\begin{align*}
  p(y \mid \theta)
    &= {10 \choose 0} \theta^0(1-\theta)^{10} + 
       {10 \choose 1} \theta^1(1-\theta)^{9} +
       {10 \choose 2} \theta^2(1-\theta)^{8} \\
    &= (1-\theta)^{10} +
       10 \theta (1-\theta)^9 +
       45 \theta^2 (1-\theta)^8
\end{align*}
$$

So the posterior density is

$$
\begin{align*}
  p(\theta \mid y) &= p(\theta) p(y \mid \theta) \\
    &\propto \theta^3(1-\theta)^{13} +
             10 \theta^4 (1-\theta)^{12} +
             45 \theta^5 (1-\theta)^{11}
\end{align*}
$$

```{r}
theta <- seq(0, 1, length.out = 100)
prior <- dbeta(theta, 4, 4)
posterior <- (dbinom(0, 10, theta) +
              dbinom(1, 10, theta) +
              dbinom(2, 10, theta)) * prior
posterior <- posterior * sum(prior) / sum(posterior)  # approximate scaling
plot(
  x = theta,
  y = prior,
  type = "l",
  lty = "dashed",
  xlab = TeX(r"($\theta$)"),
  ylab = TeX(r"($p(\theta)$)"),
  ylim = c(0, ceiling(max(prior, posterior)))
)
lines(
  x = theta,
  y = posterior,
  lty = "solid"
)
legend(
  x = "topright",
  lty = c("dashed", "solid"),
  legend = c("Prior", "Posterior")
)
```

## 2

---

**Predictive distributions: consider two coins, C1 and C2 , with the following characteristics: Pr(heads|C1 ) = 0.6 and Pr(heads|C2 ) = 0.4. Choose one of the coins at random and imagine spinning it repeatedly. Given that the first two spins from the chosen coin are tails, what is the expectation of the number of additional spins until a head shows up?**

Let the data collected of two tails be denoted as $d$.

Given that it is equally likely to pick either of the two coins (prior probability of 0.5 for both coins), the posterior probability of the spun coin being C1 is

$$
\begin{align*}
  \Pr(c_1 \mid d)
    &= \frac{\Pr(c_1) \Pr(d \mid c_1)}{\Pr(d)} \\
    &= \frac{\frac{1}{2} \times (\frac{2}{5})^2}
            {\frac{1}{2} \times (\frac{2}{5})^2 + \frac{1}{2} \times (\frac{3}{5})^2} \\
    &= \frac{4}{13}
\end{align*}
$$

So the posterior probability for C1 is $\frac{9}{13}$.

The expected number of flips until a heads for each coin is the expectation of the geometric distribution which is $\frac{1}{p}$ where $p$ is the probability of each individual spin landing on heads.

$$
E[N \mid c_1] = \frac{5}{3}
$$

$$
E[N \mid c_2] = \frac{5}{2}
$$

The total expectation is these expectations averaged over the posterior probability.

$$
\begin{align*}
  E[N \mid d] 
    &= p(c_1 \mid d) E[c_1] + p(c_2 \mid d) E[c_2] \\
    &= \frac{4}{13} \times \frac{5}{3} + \frac{9}{13} \times \frac{5}{2} \\
    &= \frac{175}{78} \\
    &\approx 2.24 
\end{align*}
$$


## 3 {.tabset}

---

**Predictive distributions: let y be the number of 6’s in 1000 rolls of a fair die.**

### a)

**Sketch the approximate distribution of y, based on the normal approximation.**

$$
\E[y] = 1000 \times \frac{1}{6} \approx 166.67
$$

$$
\Var[y] = 1000 \times \frac{1}{6} \times \frac{5}{6} \approx 138.89
$$

```{r}
y <- seq(0, 1000)
plot(
  x = y,
  y = dnorm(y, 1000/6, sqrt(1000 * (1/6) * (5/6))),
  type = "l",
  xlab = "Number of 6's Rolled in 1000 Rolls",
  ylab = "Probability",
  xlim = c(100, 250)
)
```

### b)

**Using the normal distribution table, give approximate 5%, 25%, 50%, 75%, and 95% points for the distribution of y.**

```{r, results="asis"}
quantiles <- c(0.05, 0.25, 0.5, 0.75, 0.95)
results <- qnorm(quantiles, 1000/6, sqrt(1000 * (1/6) * (5/6)))

tibble(
  "Quantile" = as.integer(quantiles * 100),
  "Value" = round(results, 2)
) |>
  kbl() |>
  kable_classic()
```

## 4 {.tabset}

---

Predictive distributions: let y be the number of 6’s in 1000 independent rolls of a particular real die, which may be unfair. Let θ be the probability that the die lands on ‘6.’ Suppose your prior distribution for θ is as follows:

Pr(θ = 1/12) = 0.25

Pr(θ = 1/6) = 0.5

Pr(θ = 1/4) = 0.25

### a)

**Using the normal approximation for the conditional distributions, p(y|θ), sketch your approximate prior predictive distribution for y.**

This is a mixture of three Gaussians.

```{r}
y <- seq(0, 1000)

thetas <- c(1/12, 1/6, 1/4)
p_y_given_theta <- lapply(thetas, \(p) dnorm(y, 1000 * p, sqrt(1000 * p * (1-p))))
prior <- c(0.25, 0.5, 0.25)

prior_predictive <- Reduce(`+`,
                           Map(`*`, p_y_given_theta, prior))

plot(
  x = y,
  y = prior_predictive,
  type = "l",
  lty = "solid",
  xlim = c(50, 300),
  xlab = "Number of 6's Rolled in 1000 Rolls",
  ylab = "Probability",
  yaxs = "i"
)
walk(Map(`*`, p_y_given_theta, prior),
     \(g) lines(y, g, lty = "dashed", col = "darkred"))
legend(
  x = "topright",
  lty = c("solid", "dashed"),
  col = c("black", "darkred"),
  legend = c("Prior Predictive", "Conditional")
)
```

### b)

We can approximate these given that the three individual Gaussian distributions have essentially no overlap.

Here are the approximate mean and standard deviations of each of the Gaussians.

$$
\E[g_1] = \frac{1000}{12} \approx 83.33 \\
\E[g_2] = \frac{1000}{6} \approx 166.67 \\
\E[g_3] = \frac{1000}{4} = 250 \\ \\
\sigma[g_1] \approx 8.74 \\
\sigma[g_2] \approx 11.78 \\
\sigma[g_3] \approx 13.69
$$

Based on the prior distributions, 25% of the probability is in the left curve, 50% is in the middle curve, and 25% is in the right curve.

So the 5% quantile is just the 20% quantile of the first curve which is `r round(qnorm(0.2, 1000 / 12, sqrt(1000 * (1/12) * (11/12))), 0)`.

The 25% quantile is between the first two curves which is about 120.

The 50% quantile is the middle of the second curve which is 167.

The 75% quantile is between the second and third curves which is about 210.

The 95% quantile is the 80% quantile of the third curve which is `r round(qnorm(0.8, 1000 / 4, sqrt(1000 * (1/4) * (3/4))), 0)`.


Another approach we could take would be to draw random samples from the distribution and use these to empirically estimate the quantiles. This approach would generalize to situations where the Gaussians were more mixed.

```{r}
samples <- sample(y, size = 10000, replace = TRUE, prob = prior_predictive)
quantile(samples, c(0.05, 0.25, 0.5, 0.75, 0.95), names = FALSE)
```

These are pretty close to the values we estimated.

```{r}
quantiles <- c(0.05, 0.25, 0.5, 0.75, 0.95)
sample_quantiles <-
  replicate(
    1000,
    {
      samples <- sample(y, size = 10000, replace = TRUE, prob = prior_predictive)
      quantile(samples, quantiles, names = FALSE)
    }
)

empirical_results <- tibble(
  "Quantile" = as.integer(quantiles * 100),
  "Median" = apply(sample_quantiles, 1, median),
  "5%" = apply(sample_quantiles, 1, \(x) quantile(x, 0.05, names = FALSE)),
  "95%" = apply(sample_quantiles, 1, \(x) quantile(x, 0.95, names = FALSE)),
  "Estimate" = c(round(qnorm(0.2, 1000 / 12, sqrt(1000 * (1/12) * (11/12))), 0),
                 120, 167, 210,
                 round(qnorm(0.8, 1000 / 4, sqrt(1000 * (1/4) * (3/4))), 0))
)

empirical_results |>
  kbl() |>
  kable_classic()
```

```{r}
ggplot(empirical_results,
       aes(y = Quantile)) +
  geom_point(aes(x = Estimate),
             color = "darkred",
             fill = "darkred",
             pch = 23) +
  geom_point(aes(x = Median)) +
  geom_segment(aes(x = `5%`,
                   xend = `95%`)) +
  theme_bw()
```

We see that our estimates were close to the empirical quantiles.
